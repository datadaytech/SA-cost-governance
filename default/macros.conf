# Search macros for User Governance app

# Get all scheduled searches with runtime metrics
# NOTE: No leading pipe - add "| " before macro call when using as first command
[get_scheduled_searches]
definition = rest /servicesNS/-/-/saved/searches splunk_server=local \
| search is_scheduled=1 \
| rename eai:acl.owner as owner, eai:acl.app as app \
| table title, owner, app, cron_schedule, disabled, dispatch.earliest_time, dispatch.latest_time, search, next_scheduled_time, qualifiedSearch \
| eval search_id = app.":".owner.":".title
iseval = 0

# Get scheduled search run history
# NOTE: No leading pipe - add "| " before macro call when using as first command
[get_search_history]
definition = rest /servicesNS/-/-/search/jobs splunk_server=local \
| search savedsearch_name=* \
| stats avg(runDuration) as avg_runtime_sec max(runDuration) as max_runtime_sec min(runDuration) as min_runtime_sec count as run_count latest(published) as last_run by savedsearch_name \
| rename savedsearch_name as title
iseval = 0

# Convert cron schedule to frequency in seconds
[parse_cron_frequency]
definition = eval frequency_seconds = case(\
    match(cron_schedule, "^\*/1 \* \* \* \*$"), 60,\
    match(cron_schedule, "^\*/5 \* \* \* \*$"), 300,\
    match(cron_schedule, "^\*/10 \* \* \* \*$"), 600,\
    match(cron_schedule, "^\*/15 \* \* \* \*$"), 900,\
    match(cron_schedule, "^\*/30 \* \* \* \*$"), 1800,\
    match(cron_schedule, "^0 \* \* \* \*$"), 3600,\
    match(cron_schedule, "^0 \*/2 \* \* \*$"), 7200,\
    match(cron_schedule, "^0 \*/4 \* \* \*$"), 14400,\
    match(cron_schedule, "^0 \*/6 \* \* \*$"), 21600,\
    match(cron_schedule, "^0 \*/12 \* \* \*$"), 43200,\
    match(cron_schedule, "^0 0 \* \* \*$"), 86400,\
    match(cron_schedule, "^0 0 \* \* 0$"), 604800,\
    1=1, 3600)\
| eval frequency_label = case(\
    frequency_seconds <= 60, "Every Minute",\
    frequency_seconds <= 300, "Every 5 Minutes",\
    frequency_seconds <= 600, "Every 10 Minutes",\
    frequency_seconds <= 900, "Every 15 Minutes",\
    frequency_seconds <= 1800, "Every 30 Minutes",\
    frequency_seconds <= 3600, "Hourly",\
    frequency_seconds <= 14400, "Every Few Hours",\
    frequency_seconds <= 86400, "Daily",\
    frequency_seconds <= 604800, "Weekly",\
    1=1, "Custom")
iseval = 0

# Calculate runtime ratio and detect suspicious patterns
[calculate_suspicious_indicators]
definition = eval runtime_ratio = if(isnotnull(avg_runtime_sec) AND frequency_seconds > 0, round((avg_runtime_sec / frequency_seconds) * 100, 2), 0)\
| eval is_high_frequency = if(frequency_seconds <= 900, 1, 0)\
| eval is_long_runtime = if(avg_runtime_sec > 300, 1, 0)\
| eval is_high_ratio = if(runtime_ratio > 10, 1, 0)\
| eval has_wasteful_commands = if(\
    match(qualifiedSearch, "index\s*=\s*\*") OR \
    match(qualifiedSearch, "\|\s*join\s") OR \
    match(qualifiedSearch, "\|\s*append\s") OR \
    match(qualifiedSearch, "\|\s*transaction\s") OR \
    match(qualifiedSearch, "earliest\s*=\s*-30d") OR \
    match(qualifiedSearch, "earliest\s*=\s*-60d") OR \
    match(qualifiedSearch, "earliest\s*=\s*-90d"), 1, 0)\
| eval is_suspicious = if(is_high_frequency=1 OR is_long_runtime=1 OR is_high_ratio=1 OR has_wasteful_commands=1, 1, 0)\
| eval suspicious_reason = mvappend(\
    if(is_high_ratio=1, "Runtime exceeds ".tostring(runtime_ratio)."% of schedule interval", null()),\
    if(is_high_frequency=1, "High frequency schedule (".frequency_label.")", null()),\
    if(is_long_runtime=1, "Long average runtime (".tostring(round(avg_runtime_sec/60, 1))." min)", null()),\
    if(has_wasteful_commands=1, "Contains potentially wasteful SPL commands", null()))\
| eval suspicious_reason = mvjoin(suspicious_reason, "; ")
iseval = 0

# Full scheduled search analysis
# NOTE: No leading pipe - use "| `analyze_scheduled_searches`" in searches
[analyze_scheduled_searches]
definition = rest /servicesNS/-/-/saved/searches splunk_server=local \
| search is_scheduled=1 \
| rename eai:acl.owner as owner, eai:acl.app as app \
| table title, owner, app, cron_schedule, disabled, dispatch.earliest_time, dispatch.latest_time, search, next_scheduled_time, qualifiedSearch \
| eval search_id = app.":".owner.":".title \
| join type=left title [| `get_search_history`] \
| `parse_cron_frequency` \
| `calculate_suspicious_indicators` \
| eval avg_runtime_display = if(isnotnull(avg_runtime_sec), tostring(round(avg_runtime_sec, 1))."s", "N/A") \
| eval max_runtime_display = if(isnotnull(max_runtime_sec), tostring(round(max_runtime_sec, 1))."s", "N/A") \
| sort - is_suspicious, - runtime_ratio
iseval = 0

# Get flagged searches from KV store
# NOTE: No leading pipe - use "| `get_flagged_searches`" in searches
[get_flagged_searches]
definition = inputlookup flagged_searches_lookup \
| eval days_remaining = round((remediation_deadline - now()) / 86400, 1) \
| eval flagged_date = strftime(flagged_time, "%Y-%m-%d %H:%M") \
| eval deadline_date = strftime(remediation_deadline, "%Y-%m-%d %H:%M") \
| eval status_display = case(\
    status="pending", "Pending Notification",\
    status="notified", "Awaiting Remediation",\
    status="disabled", "Disabled",\
    status="resolved", "Resolved",\
    1=1, status)
iseval = 0

# Get governance settings with defaults
# NOTE: No leading pipe - use "| `get_governance_settings`" in searches
[get_governance_settings]
definition = inputlookup governance_settings_lookup \
| append [| makeresults | eval setting_name="runtime_ratio_threshold", setting_value="10", description="Maximum runtime as percentage of schedule interval" | fields - _time] \
| append [| makeresults | eval setting_name="high_frequency_threshold", setting_value="900", description="Minimum seconds between runs to flag as high frequency" | fields - _time] \
| append [| makeresults | eval setting_name="long_runtime_threshold", setting_value="300", description="Runtime in seconds to flag as long running" | fields - _time] \
| append [| makeresults | eval setting_name="remediation_days", setting_value="7", description="Days allowed for remediation before auto-disable" | fields - _time] \
| dedup setting_name
iseval = 0

# Audit log entry macro
[log_governance_action(4)]
args = action, search_name, performed_by, details
definition = makeresults \
| eval timestamp=now(), action="$action$", search_name="$search_name$", performed_by="$performed_by$", details="$details$" \
| table timestamp, action, search_name, performed_by, details \
| outputlookup append=true governance_audit_log_lookup
iseval = 0

# ============================================
# COST CALCULATION MACROS (SVC-BASED)
# ============================================

# Get cost settings from governance_settings_lookup with defaults
[get_cost_settings]
definition = inputlookup governance_settings_lookup \
| append [| makeresults | eval setting_name="svc_unit_cost", setting_value="1600", description="Annual cost per SVC in dollars"] \
| append [| makeresults | eval setting_name="svc_purchased", setting_value="10000", description="SVC Entitlement (annual)"] \
| append [| makeresults | eval setting_name="ddas_unit_cost", setting_value="0.05", description="Cost per DDAS query"] \
| append [| makeresults | eval setting_name="ddaa_unit_cost", setting_value="0.15", description="Cost per DDAA operation"] \
| append [| makeresults | eval setting_name="ingest_gb_cost", setting_value="15.00", description="Cost per GB ingested"] \
| fields - _time \
| dedup setting_name \
| stats values(eval(if(setting_name="svc_unit_cost", setting_value, null()))) as svc_unit_cost \
        values(eval(if(setting_name="svc_purchased", setting_value, null()))) as svc_purchased \
        values(eval(if(setting_name="ddas_unit_cost", setting_value, null()))) as ddas_unit_cost \
        values(eval(if(setting_name="ddaa_unit_cost", setting_value, null()))) as ddaa_unit_cost \
        values(eval(if(setting_name="ingest_gb_cost", setting_value, null()))) as ingest_gb_cost
iseval = 0

# Mock SVC usage based on search characteristics
# Regular searches: ~15-30 SVC, Expensive high-frequency: ~80-120 SVC
[calculate_mock_svc_usage]
definition = eval mock_svc_base = case( \
    frequency_seconds <= 300 AND avg_runtime_sec > 60, random() % 40 + 80, \
    frequency_seconds <= 300, random() % 30 + 70, \
    frequency_seconds <= 900 AND avg_runtime_sec > 120, random() % 35 + 85, \
    frequency_seconds <= 900, random() % 25 + 60, \
    match(qualifiedSearch, "earliest\s*=\s*-30d|earliest\s*=\s*-60d|earliest\s*=\s*-90d"), random() % 40 + 75, \
    match(qualifiedSearch, "index\s*=\s*\*"), random() % 50 + 70, \
    match(qualifiedSearch, "\|\s*join\s|\|\s*transaction\s"), random() % 30 + 65, \
    avg_runtime_sec > 180, random() % 30 + 50, \
    avg_runtime_sec > 60, random() % 20 + 25, \
    1=1, random() % 15 + 15) \
| eval svc_per_run = mock_svc_base
iseval = 0

# Calculate cost metrics for scheduled searches (SVC-based)
# Formula: monthly_cost = monthly_svc × (annual_svc_cost / 12)
# Example: 100 SVCs/month × ($1600/12) = 100 × $133.33 = $13,333/month
[calculate_search_costs]
definition = eval runs_per_day = if(frequency_seconds > 0, round(86400 / frequency_seconds, 2), 0) \
| eval runs_per_month = round(runs_per_day * 30, 0) \
| eval avg_runtime_sec = if(isnotnull(avg_runtime_sec), avg_runtime_sec, 30) \
| `calculate_mock_svc_usage` \
| eval monthly_svc_usage = round(runs_per_month * svc_per_run, 0) \
| eval svc_annual_cost = 1600 \
| eval monthly_cost_per_svc = svc_annual_cost / 12 \
| eval monthly_svc_cost = round(monthly_svc_usage * monthly_cost_per_svc, 2) \
| eval monthly_total_cost = monthly_svc_cost \
| eval annual_cost = round(monthly_total_cost * 12, 2) \
| eval cost_tier = case( \
    monthly_total_cost >= 50000, "Critical", \
    monthly_total_cost >= 20000, "High", \
    monthly_total_cost >= 5000, "Medium", \
    monthly_total_cost >= 1000, "Low", \
    1=1, "Minimal") \
| eval cost_display = "$".tostring(round(monthly_total_cost, 0)) \
| eval svc_display = tostring(monthly_svc_usage)." SVCs"
iseval = 0

# Full cost analysis for all scheduled searches
# Note: Removed expensive join on search/jobs for performance. Runtime estimates are approximated.
[analyze_search_costs]
definition = rest /servicesNS/-/-/saved/searches splunk_server=local \
| search is_scheduled=1 \
| rename eai:acl.owner as owner, eai:acl.app as app \
| table title, owner, app, cron_schedule, disabled, dispatch.earliest_time, dispatch.latest_time, qualifiedSearch \
| eval avg_runtime_sec = 30 \
| eval run_count = 0 \
| `parse_cron_frequency` \
| `calculate_search_costs` \
| sort - monthly_total_cost
iseval = 0

# Load cached cost analysis results from stored job SID
# This reads from the previously cached search job without re-running the REST call
[load_cached_cost_analysis]
definition = inputlookup governance_cache_sid_lookup \
| head 1 \
| map search="| loadjob $cache_sid$"
iseval = 0

# Get the cached job SID for cost analysis
[get_cost_cache_sid]
definition = inputlookup governance_cache_sid_lookup | head 1 | return $cache_sid
iseval = 0

# Calculate savings from disabled/flagged searches (SVC-based)
# Uses same formula: monthly_cost = monthly_svc × ($1600 / 12)
[calculate_governance_savings]
definition = inputlookup flagged_searches_lookup \
| search status="disabled" \
| eval mock_svc_saved = random() % 60 + 60 \
| eval runs_per_month_saved = 720 \
| eval monthly_svc_saved = mock_svc_saved * runs_per_month_saved \
| eval svc_annual_cost = 1600 \
| eval monthly_cost_per_svc = svc_annual_cost / 12 \
| eval saved_monthly_cost = round(monthly_svc_saved * monthly_cost_per_svc, 2) \
| stats sum(saved_monthly_cost) as total_monthly_savings sum(monthly_svc_saved) as total_svc_saved count as disabled_count \
| eval total_annual_savings = total_monthly_savings * 12 \
| eval savings_display = "$".tostring(round(total_monthly_savings, 0))."/month (".tostring(total_svc_saved)." SVCs)"
iseval = 0

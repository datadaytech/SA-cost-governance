#
# Splunk Environment Diagnostics - Saved Searches
#
# These searches run sequentially (staggered 5-min cron) to collect
# diagnostic metrics about the Splunk environment.
#
# Search Chain:
#   00 - Initialize (clear previous, set run_id)
#   01 - Search Inventory (total, scheduled, ad hoc, dashboard)
#   02 - Data Model Acceleration status
#   03 - SVC by Search
#   04 - SVC Peak Windows
#   05 - SVC Rolling Averages
#   06 - Schedule Distribution (minute-of-hour skew)
#   07 - Runtime Gap Analysis
#   08 - User Activity
#   09 - License Health
#   10 - Indexing Queue Health
#   11 - Storage Metrics
#   99 - Finalize (export + webhook/email)
#

# =============================================================================
# SEARCH 00: Initialize Diagnostic Run
# =============================================================================
[diag_00_initialize]
description = Clears previous results and initializes a new diagnostic run with unique run_id. Detects Cloud vs Enterprise environment.
search = | makeresults \
    | eval run_id=`generate_run_id` \
    | eval start_time=now(), status="running", search_count=0 \
    | table run_id, start_time, status, search_count \
    | outputlookup diagnostic_meta_lookup \
    | append \
        [| inputlookup diagnostic_results_lookup \
         | delete] \
    | append \
        [| `detect_environment` \
         | append [| `has_introspection`] \
         | append [| rest /services/server/info splunk_server=local | fields version, os_name, cpu_arch, numberOfCores, physicalMemoryMB | head 1] \
         | stats values(*) as * \
         | eval category="meta", metric_name="environment_info" \
         | eval metric_value=environment \
         | eval details=json_object( \
             "environment", environment, \
             "has_introspection", has_introspection, \
             "splunk_version", version, \
             "os_name", os_name, \
             "cpu_arch", cpu_arch, \
             "cores", numberOfCores, \
             "memory_mb", physicalMemoryMB) \
         | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_00_initialize" \
         | `diag_output_fields` \
         | `diag_output_to_kvstore`] \
    | stats count as initialized \
    | eval message="Diagnostic run initialized" \
    | table message

cron_schedule = 0 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -1m
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 01: Search Inventory
# =============================================================================
[diag_01_search_inventory]
description = Counts total searches by type (scheduled, ad hoc, dashboard, data model)
search = index=_internal sourcetype=scheduler OR sourcetype=splunkd_access earliest=`diag_earliest` latest=`diag_latest` \
    | eval search_type=case( \
        searchmatch("sourcetype=scheduler"), "scheduled", \
        searchmatch("splunkd_access") AND match(_raw, "\/servicesNS\/.*\/search\/jobs"), "ad_hoc", \
        searchmatch("splunkd_access") AND match(_raw, "\/splunkd\/.*dashboard"), "dashboard", \
        1==1, "other") \
    | stats count as search_count by search_type \
    | eval total=search_count \
    | append \
        [| rest /services/saved/searches splunk_server=local \
         | stats count as total_saved_searches \
         | eval search_type="saved_search_definitions", search_count=total_saved_searches] \
    | eval category="inventory", metric_name="search_count_by_type" \
    | eval metric_value=search_type \
    | eval details=json_object("count", search_count, "type", search_type) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_01_search_inventory" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 5 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 02: Data Model Acceleration Status
# =============================================================================
[diag_02_datamodel_acceleration]
description = Reports on data model acceleration status and search counts
search = | rest /services/data/models splunk_server=local \
    | rename eai:acl.app as app, acceleration.enabled as accel_enabled \
    | eval accel_status=if(accel_enabled="1", "enabled", "disabled") \
    | stats count as model_count by app, accel_status \
    | append \
        [index=_internal source=*scheduler.log* earliest=`diag_earliest` latest=`diag_latest` \
         | search savedsearch_name="*_ACCELERATE_*" OR savedsearch_name="*DM_*" \
         | stats count as accel_search_runs by savedsearch_name \
         | head 20] \
    | eval category="datamodel", metric_name=if(isnotnull(savedsearch_name), "acceleration_search_runs", "model_status") \
    | eval metric_value=coalesce(savedsearch_name, accel_status) \
    | eval details=json_object("count", coalesce(accel_search_runs, model_count), "app", app, "status", accel_status) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_02_datamodel_acceleration" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 10 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 03: SVC by Search (Cloud + Enterprise compatible)
# =============================================================================
[diag_03_svc_by_search]
description = Calculates SVC/resource consumption per saved search (supports Cloud and Enterprise)
search = | eventcount summarize=false index=_cmc_summary \
    | eval has_cmc=if(count>0, 1, 0) \
    | map maxsearches=1 search="| makeresults | eval source_type=if($has_cmc$=1, \"cloud\", \"enterprise\")" \
    | map maxsearches=1 search=" \
        [| search index=_cmc_summary source=*search_usage* earliest=`diag_earliest` latest=`diag_latest` \
         | stats sum(svc) as total_svc, max(svc) as max_svc, avg(svc) as avg_svc, count as run_count by savedsearch_name, app \
         | eval data_source=\"cloud_cmc_summary\" \
         | where isnotnull(total_svc)] \
        | append \
        [| search index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=`diag_earliest` latest=`diag_latest` data.search_props.sid=* \
         | rename data.search_props.label as savedsearch_name, data.search_props.app as app \
         | rename data.mem_used as mem_used, data.elapsed as elapsed, data.pct_cpu as pct_cpu \
         | where isnotnull(savedsearch_name) AND savedsearch_name!=\"\" \
         | eval svc_estimate=round((elapsed * pct_cpu / 100), 2) \
         | stats sum(svc_estimate) as total_svc, max(svc_estimate) as max_svc, avg(svc_estimate) as avg_svc, count as run_count, avg(mem_used) as avg_mem_mb by savedsearch_name, app \
         | eval data_source=\"enterprise_introspection\" \
         | where isnotnull(total_svc)] \
        | append \
        [| search index=_internal sourcetype=scheduler status=success earliest=`diag_earliest` latest=`diag_latest` \
         | eval svc_estimate=run_time * (result_count/1000 + 1) \
         | stats sum(svc_estimate) as total_svc, max(svc_estimate) as max_svc, avg(svc_estimate) as avg_svc, count as run_count, sum(run_time) as total_runtime_sec by savedsearch_name, app \
         | eval data_source=\"enterprise_scheduler\" \
         | where isnotnull(total_svc)] \
        | eventstats dc(data_source) as source_count by savedsearch_name \
        | where (data_source=\"cloud_cmc_summary\") OR (data_source=\"enterprise_introspection\" AND source_count=1) OR (data_source=\"enterprise_scheduler\" AND source_count=1) \
        | dedup savedsearch_name \
        | sort - total_svc \
        | head 100" \
    | eval category="svc", metric_name="svc_by_search" \
    | eval metric_value=savedsearch_name \
    | eval details=json_object( \
        "total_svc", round(total_svc,2), \
        "max_svc", round(max_svc,2), \
        "avg_svc", round(avg_svc,2), \
        "run_count", run_count, \
        "app", app, \
        "data_source", data_source, \
        "avg_mem_mb", round(avg_mem_mb,2), \
        "total_runtime_sec", round(total_runtime_sec,0)) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_03_svc_by_search" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 15 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 04: SVC Peak Windows (Cloud + Enterprise compatible)
# =============================================================================
[diag_04_svc_peaks]
description = Identifies peak SVC/resource consumption windows in 15-minute intervals
search = index=_cmc_summary source=*search_usage* earliest=`diag_earliest` latest=`diag_latest` \
    | eval data_source="cloud" \
    | append \
        [| search index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=`diag_earliest` latest=`diag_latest` \
         | rename data.elapsed as elapsed, data.pct_cpu as pct_cpu \
         | eval svc=round((elapsed * pct_cpu / 100), 2) \
         | eval data_source="enterprise_introspection"] \
    | append \
        [| search index=_internal sourcetype=scheduler status=success earliest=`diag_earliest` latest=`diag_latest` \
         | eval svc=run_time \
         | eval data_source="enterprise_scheduler"] \
    | bin _time span=15m \
    | stats sum(svc) as svc_sum, count as search_count, values(data_source) as sources by _time \
    | eval data_source=mvindex(sources, 0) \
    | sort - svc_sum \
    | head 50 \
    | eval hour_of_day=strftime(_time, "%H:%M") \
    | eval day_of_week=strftime(_time, "%A") \
    | eval category="svc", metric_name="svc_peak_windows" \
    | eval metric_value=strftime(_time, "%Y-%m-%d %H:%M") \
    | eval details=json_object( \
        "svc_sum", round(svc_sum,2), \
        "search_count", search_count, \
        "hour_of_day", hour_of_day, \
        "day_of_week", day_of_week, \
        "data_source", data_source) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_04_svc_peaks" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 20 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 05: SVC Rolling Averages (Cloud + Enterprise compatible)
# =============================================================================
[diag_05_svc_rolling_avg]
description = Calculates rolling hourly SVC/resource averages over time
search = index=_cmc_summary source=*search_usage* earliest=`diag_earliest` latest=`diag_latest` \
    | eval data_source="cloud" \
    | append \
        [| search index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=`diag_earliest` latest=`diag_latest` \
         | rename data.elapsed as elapsed, data.pct_cpu as pct_cpu, data.mem_used as mem_used \
         | eval svc=round((elapsed * pct_cpu / 100), 2) \
         | eval data_source="enterprise_introspection"] \
    | append \
        [| search index=_internal sourcetype=scheduler status=success earliest=`diag_earliest` latest=`diag_latest` \
         | eval svc=run_time \
         | eval data_source="enterprise_scheduler"] \
    | timechart span=1h avg(svc) as svc_avg, max(svc) as svc_max, sum(svc) as svc_total \
    | streamstats window=24 avg(svc_avg) as rolling_24h_avg, avg(svc_max) as rolling_24h_max \
    | stats \
        avg(svc_avg) as overall_avg_svc, \
        max(svc_max) as overall_max_svc, \
        avg(rolling_24h_avg) as avg_rolling_24h, \
        max(rolling_24h_max) as max_rolling_24h, \
        sum(svc_total) as total_svc_period \
    | eval category="svc", metric_name="svc_rolling_summary" \
    | eval metric_value="7_day_summary" \
    | eval details=json_object( \
        "overall_avg_svc", round(overall_avg_svc,2), \
        "overall_max_svc", round(overall_max_svc,2), \
        "avg_rolling_24h", round(avg_rolling_24h,2), \
        "max_rolling_24h", round(max_rolling_24h,2), \
        "total_svc_period", round(total_svc_period,2), \
        "note", "Enterprise uses estimated SVC from runtime*cpu; Cloud uses actual SVC") \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_05_svc_rolling_avg" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 25 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 06: Schedule Distribution (Minute-of-Hour Skew)
# =============================================================================
[diag_06_schedule_distribution]
description = Analyzes scheduled search distribution by minute-of-hour to identify skew
search = index=_internal sourcetype=scheduler earliest=`diag_earliest` latest=`diag_latest` status=success \
    | eval minute=strftime(_time, "%M") \
    | stats count as search_count by minute \
    | sort minute \
    | eval category="schedule", metric_name="minute_distribution" \
    | eval minute_int=tonumber(minute) \
    | eval is_hotspot=if(minute_int=0 OR minute_int=15 OR minute_int=30 OR minute_int=45, "yes", "no") \
    | eval metric_value=minute \
    | eval details=json_object("count", search_count, "minute", minute, "is_hotspot", is_hotspot) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_06_schedule_distribution" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 30 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 07: Runtime Gap Analysis
# =============================================================================
[diag_07_runtime_gap_analysis]
description = Identifies gaps between search runtime and lookback window
search = index=_internal sourcetype=scheduler earliest=`diag_earliest` latest=`diag_latest` status=success run_time>0 \
    | eval runtime_seconds=run_time \
    | stats avg(runtime_seconds) as avg_runtime, max(runtime_seconds) as max_runtime, count as runs by savedsearch_name \
    | join type=left savedsearch_name \
        [| rest /services/saved/searches splunk_server=local \
         | rename title as savedsearch_name \
         | eval cron=cron_schedule \
         | eval dispatch_earliest=if(isnotnull('dispatch.earliest_time'), 'dispatch.earliest_time', "-1h") \
         | eval lookback_match=if(match(dispatch_earliest, "^-(\d+)([smhd])"), dispatch_earliest, null()) \
         | rex field=lookback_match "^-(?<lookback_value>\d+)(?<lookback_unit>[smhd])" \
         | eval lookback_seconds=case( \
             lookback_unit="s", lookback_value, \
             lookback_unit="m", lookback_value*60, \
             lookback_unit="h", lookback_value*3600, \
             lookback_unit="d", lookback_value*86400, \
             1==1, 3600) \
         | fields savedsearch_name, cron, lookback_seconds, dispatch_earliest] \
    | where isnotnull(lookback_seconds) \
    | eval gap_seconds=avg_runtime - lookback_seconds \
    | eval gap_ratio=round(avg_runtime/lookback_seconds, 2) \
    | eval has_gap=if(gap_ratio > 0.8, "warning", if(gap_ratio > 1, "critical", "ok")) \
    | where has_gap!="ok" \
    | sort - gap_ratio \
    | head 50 \
    | eval category="schedule", metric_name="runtime_gap_analysis" \
    | eval metric_value=savedsearch_name \
    | eval details=json_object( \
        "avg_runtime_sec", round(avg_runtime,1), \
        "max_runtime_sec", round(max_runtime,1), \
        "lookback_seconds", lookback_seconds, \
        "gap_ratio", gap_ratio, \
        "status", has_gap, \
        "runs", runs) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_07_runtime_gap_analysis" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 35 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 08: User Activity
# =============================================================================
[diag_08_user_activity]
description = Analyzes user search activity and identifies top querying users
search = index=_audit action=search earliest=`diag_earliest` latest=`diag_latest` \
    | stats count as search_count, dc(search) as unique_searches, \
            values(app) as apps_used, latest(_time) as last_search by user \
    | sort - search_count \
    | head 50 \
    | eval last_search_time=strftime(last_search, "%Y-%m-%d %H:%M:%S") \
    | eval category="users", metric_name="user_activity" \
    | eval metric_value=user \
    | eval details=json_object( \
        "search_count", search_count, \
        "unique_searches", unique_searches, \
        "apps_used", mvjoin(apps_used, ","), \
        "last_search", last_search_time) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_08_user_activity" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore` \
    | append \
        [| search index=_audit action=search earliest=`diag_earliest` latest=`diag_latest` \
         | stats dc(user) as unique_users, count as total_searches \
         | eval category="users", metric_name="user_summary" \
         | eval metric_value="total" \
         | eval details=json_object("unique_users", unique_users, "total_searches", total_searches) \
         | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_08_user_activity" \
         | `diag_output_fields` \
         | `diag_output_to_kvstore`]

cron_schedule = 40 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 09: License Health
# =============================================================================
[diag_09_license_health]
description = Reports on license usage and violations
search = index=_internal sourcetype=splunkd component=LicenseUsage earliest=`diag_earliest` latest=`diag_latest` \
    | eval date=strftime(_time, "%Y-%m-%d") \
    | stats sum(b) as bytes_indexed by date, pool \
    | eval gb_indexed=round(bytes_indexed/1024/1024/1024, 2) \
    | append \
        [| rest /services/licenser/pools splunk_server=local \
         | eval quota_gb=round(effective_quota/1024/1024/1024, 2) \
         | fields title, quota_gb, used_bytes \
         | rename title as pool \
         | eval current_used_gb=round(used_bytes/1024/1024/1024, 2)] \
    | append \
        [| search index=_internal sourcetype=splunkd "license violation" earliest=`diag_earliest` latest=`diag_latest` \
         | stats count as violation_count by date_mday \
         | eval is_violation="true"] \
    | eval category="license", metric_name=case( \
        isnotnull(is_violation), "license_violations", \
        isnotnull(quota_gb), "license_quota", \
        1==1, "daily_usage") \
    | eval metric_value=coalesce(date, pool, "violations") \
    | eval details=json_object( \
        "gb_indexed", gb_indexed, \
        "quota_gb", quota_gb, \
        "current_used_gb", current_used_gb, \
        "violation_count", violation_count, \
        "pool", pool) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_09_license_health" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 45 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 10: Indexing Queue Health
# =============================================================================
[diag_10_indexing_queue]
description = Monitors indexing queue depths and identifies throttling issues
search = index=_internal sourcetype=splunkd component=Metrics group=queue earliest=`diag_earliest` latest=`diag_latest` \
    | stats max(current_size) as max_queue_size, avg(current_size) as avg_queue_size, \
            max(largest_size) as largest_ever by name \
    | append \
        [| search index=_internal sourcetype=splunkd component=Metrics group=thruput earliest=`diag_earliest` latest=`diag_latest` \
         | timechart span=1h avg(instantaneous_kbps) as avg_kbps, max(instantaneous_kbps) as max_kbps \
         | stats avg(avg_kbps) as overall_avg_kbps, max(max_kbps) as peak_kbps] \
    | append \
        [| search index=_internal sourcetype=splunkd "throttl" earliest=`diag_earliest` latest=`diag_latest` \
         | stats count as throttle_events] \
    | eval category="indexing", metric_name=case( \
        isnotnull(name), "queue_health", \
        isnotnull(overall_avg_kbps), "throughput", \
        isnotnull(throttle_events), "throttling") \
    | eval metric_value=coalesce(name, "throughput", "throttle_count") \
    | eval details=json_object( \
        "max_queue_size", max_queue_size, \
        "avg_queue_size", round(avg_queue_size,2), \
        "largest_ever", largest_ever, \
        "overall_avg_kbps", round(overall_avg_kbps,2), \
        "peak_kbps", round(peak_kbps,2), \
        "throttle_events", throttle_events) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_10_indexing_queue" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 50 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 11: Storage Metrics
# =============================================================================
[diag_11_storage_metrics]
description = Reports on index storage usage and DDA/DDAS metrics
search = | rest /services/data/indexes splunk_server=local \
    | eval size_gb=round(currentDBSizeMB/1024, 2) \
    | eval max_size_gb=round(maxTotalDataSizeMB/1024, 2) \
    | eval usage_pct=round((currentDBSizeMB/maxTotalDataSizeMB)*100, 1) \
    | stats sum(size_gb) as total_size_gb, sum(max_size_gb) as total_max_gb, \
            values(eval(if(usage_pct>80, title.":".usage_pct."%", null()))) as high_usage_indexes by splunk_server \
    | append \
        [| rest /services/data/indexes splunk_server=local \
         | where datatype="metric" OR match(title, "^_") \
         | eval size_gb=round(currentDBSizeMB/1024, 2) \
         | stats count as index_count, sum(size_gb) as total_size by datatype \
         | eval index_type=if(datatype="metric", "metrics_indexes", "internal_indexes")] \
    | append \
        [| dbinspect index=* splunk_server=local \
         | stats sum(sizeOnDiskMB) as total_disk_mb by state \
         | eval state_size_gb=round(total_disk_mb/1024, 2)] \
    | eval category="storage", metric_name=case( \
        isnotnull(total_size_gb), "storage_by_server", \
        isnotnull(index_type), "storage_by_type", \
        isnotnull(state), "storage_by_state") \
    | eval metric_value=coalesce(splunk_server, index_type, state) \
    | eval details=json_object( \
        "total_size_gb", total_size_gb, \
        "total_max_gb", total_max_gb, \
        "high_usage_indexes", mvjoin(high_usage_indexes, ","), \
        "index_count", index_count, \
        "state_size_gb", state_size_gb) \
    | eval run_id=`get_current_run_id`, run_time=now(), search_name="diag_11_storage_metrics" \
    | `diag_output_fields` \
    | `diag_output_to_kvstore`

cron_schedule = 55 2 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic


# =============================================================================
# SEARCH 99: Finalize and Export
# =============================================================================
[diag_99_finalize]
description = Exports all results to CSV and triggers webhook/email notification
search = | inputlookup diagnostic_meta_lookup \
    | where status="running" \
    | eval end_time=now() \
    | eval duration_seconds=end_time-start_time \
    | eval status="completed" \
    | outputlookup diagnostic_meta_lookup \
    | append \
        [| inputlookup diagnostic_results_lookup \
         | stats count as total_metrics, dc(category) as categories, dc(search_name) as searches_completed by run_id] \
    | table run_id, start_time, end_time, duration_seconds, status, total_metrics, categories, searches_completed \
    | append \
        [| inputlookup diagnostic_results_lookup \
         | outputlookup diagnostic_results_export.csv] \
    | head 1

cron_schedule = 0 3 * * *
enableSched = 1
disabled = 1
dispatch.earliest_time = -1h
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic

action.webhook = 1
action.webhook.param.url =
action.email = 0
action.email.to =
action.email.subject = Splunk Environment Diagnostics Report - $result.run_id$
action.email.format = csv
action.email.include.results_link = 1
action.email.inline = 1


# =============================================================================
# MANUAL TRIGGER: Run Full Diagnostics Now
# =============================================================================
[Run Diagnostics Now]
description = Manually triggers a full diagnostic run immediately
search = | savedsearch diag_00_initialize

dispatch.earliest_time = -1m
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic
disabled = 0


# =============================================================================
# VIEW: Latest Diagnostic Results
# =============================================================================
[View Latest Diagnostics]
description = View the most recent diagnostic results
search = | inputlookup diagnostic_results_lookup \
    | join type=left run_id \
        [| inputlookup diagnostic_meta_lookup | fields run_id, status, start_time, end_time] \
    | eval run_date=strftime(start_time, "%Y-%m-%d %H:%M:%S") \
    | stats count by run_id, run_date, status, category, metric_name

dispatch.earliest_time = -1m
dispatch.latest_time = now
is_visible = true
request.ui_dispatch_app = TA-splunk_diagnostic
disabled = 0
